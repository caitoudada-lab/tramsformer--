LLM(Large Language Model)，即大型语言模型，基于深度学习的人工智能模型，可以理解人类的语言。
主要特点是 参数规模大（数亿到数万亿参数。）
通过海量文本数据进行训练，具备语言理解、生成和逻辑推理的能力。


所谓大型，就体现在参数量上，例如GPT-3、GPT-4、PaLM 2(Google)，都有千亿级的数据量，甚至万亿级。


训练方式包括，预训练、微调、提示工程。

预训练：海量 无标注的数据上学习通用的语言学习能力，如预测句子中的下一个词（掩码语言模型）、理解上下文关系（Transformer架构）。Pretraining

微调：指  针对特定任务（翻译、问答、代码生成），使用 少量，标记过的数据调整预训练得到的模型，快速适配新的场景。Fine-tuning

提示工程：通过设计高质量的文本提示（Prompt），让模型在无需微调的情况下完成复杂的任务。


关键技术：Transformer架构、涌现能力（Emergent Abilities）、多模态融合。

Transformer架构，17年提出，核心优势是通过注意力机制，捕捉文本 中远 距离的依赖关系。
改进版本出现在，GPT(只使用解码器)、BERT(只使用编码器)、T5(编码器-解码器)。

注意：编码器是将输入序列转换成一种内部表示，可以是固定长度的向量或者一系列向量，这个内部表示称为编码表示。
      解码器是指 接收编码器生成的编码表示，以及之前的输出（如果存在的话），逐步生成目标序列。
      不同技术只是用一种或多种，也就注定了它自身的优势和局限。

涌现能力，属于是灵光乍现。。。
常指模型参数规模超过某个阈值，会  突然性的获得预训练数据中未明确学习的能力，即学习了预期外的能力。

多模态融合，部分LLM 支持多模态输入，结合文本、图像、语言等信息进行处理。

注：新概念：幻觉，通常指模型生成了不符合事实的内容。Hallucination






下接训练方式中的   提示词工程：

定义不在重复。

在其中，“角色-任务-要求”三段式结构 是一种系统化的  设计提示的方法，通过明确三个核心模块，让LLM更好理解用户需求。
使用这个结构可以 显著减少歧义，提升输出的可控性。


三段式结构详解：

角色：定义模型的“身份”或者“视角”，引导它以指定的方式输出。比如：“你是一个找实习工作的研究生牛马，下面请。。”
作用：选定模型的专业领域，避免泛化回答。

任务：注意是  明确告知  模型的具体任务

要求定义：细分任务的约束条件，比如格式、风格、长度等



提示词工程的核心原则：
清晰性与具体性、结构化提示设计、示例引导、引导逻辑推理、控制输出格式、调整模型创造性
上述原则总结：通过设计和调整输入的提示词

